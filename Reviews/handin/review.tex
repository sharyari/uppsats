\documentclass[a4paper,10pt]{article}
\usepackage{fullpage}
\usepackage[british]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%\usepackage{amsthm} \newtheorem{theorem}{Theorem}
\usepackage{color}

\usepackage{float}
\usepackage{enumerate}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}

\usepackage{alltt}
\usepackage{listings}
 \usepackage{aeguill}
\usepackage{dsfont}
%\usepackage{algorithm}
\usepackage[noend]{algorithm2e}
%\usepackage{algorithmicx}
\usepackage{subfig}
\lstset{% parameters for all code listings
language=Python,
frame=single,
basicstyle=\small, % nothing smaller than \footnotesize, please
tabsize=2,
numbers=left,
% framexleftmargin=2em, % extend frame to include line numbers
%xrightmargin=2em, % extra space to fit 79 characters
breaklines=true,
breakatwhitespace=true,
prebreak={/},
captionpos=b,
columns=fullflexible,
escapeinside={\#*}{\^^M}
}


% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    % General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    % Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4} % 2 may work better
    \setcounter{dbltopnumber}{2} % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    % Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

% remember to use [htp] or [htpb] for placement


\usepackage{fancyvrb}
%\DefineVerbatimEnvironment{code}{Verbatim}{fontsize=\small}
%\DefineVerbatimEnvironment{example}{Verbatim}{fontsize=\small}

\usepackage{url}
\urldef{\mailsa}\path|josh0151@student.uu.se |
\urldef{\mailsb}\path|bjfo5755@student.uu.se |
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}


\usepackage{tikz} \usetikzlibrary{trees}
\usepackage{hyperref} % should always be the last package

% useful colours (use sparingly!):
\newcommand{\blue}[1]{{\color{blue}#1}}
\newcommand{\green}[1]{{\color{green}#1}}
\newcommand{\red}[1]{{\color{red}#1}}

% useful wrappers for algorithmic/Python notation:
\newcommand{\length}[1]{\text{len}(#1)}
\newcommand{\twodots}{\mathinn\usepackage{enumerate}er{\ldotp\ldotp}} % taken from clrscode3e.sty
\newcommand{\Oh}[1]{\mathcal{O}\left(#1\right)}

% useful (wrappers for) math symbols:
\newcommand{\Cardinality}[1]{\left\lvert#1\right\rvert}
%\newcommand{\Cardinality}[1]{\##1}
\newcommand{\Ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\Floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\Iff}{\Leftrightarrow}
\newcommand{\Implies}{\Rightarrow}
\newcommand{\Intersect}{\cap}
\newcommand{\Sequence}[1]{\left[#1\right]}
\newcommand{\Set}[1]{\left\{#1\right\}}
\newcommand{\SetComp}[2]{\Set{#1\SuchThat#2}}
\newcommand{\SuchThat}{\mid}
\newcommand{\Tuple}[1]{\langle#1\rangle}
\newcommand{\Union}{\cup}
\usetikzlibrary{positioning,shapes,shadows,arrows}

\usepackage{url}

\pagestyle{empty}

\title{Review of Facial-Feature Based \\
	Human-Computer Interface \\
	For Disabled People \\
	\vspace{3mm} \normalsize Original paper by: K. Parmar, B.Mehta, R. Sawant}

\author{Bj{\"o}rn Forsberg, Jonathan Sharyari}
\oddsidemargin 0.5in 
\textwidth 5.5in 
\begin{document}


\maketitle


\section{Abstract}

The traditional methods for human computer interaction might be unavailable to persons with physical handicaps. For these persons other means of interacting with a computer is needed. The paper describes a gaze communication interface, where the users use facial features to control the computer. This is done by using the SSR algorithm to detect the face of the user and controlling the mouse cursor via eye and nose tip movements. The article focuses mostly on the technical base of the system,  implemented and tested on a sample group. The success rate and accuracy of the system is investigated based on user movement, light conditions, background noise, involuntary blinking and video frame rate.
The goal is to create a system that can run in real time on a standard desktop computer with a high enough accuracy rate for regular use. The system only uses the built in web cam of most modern displays and works with minimal configuration.

\section{Content}
The article\cite{thepaper} describes an input method especially designed for disabled people, with the ability to voluntarily control their eye movement and blinking. This is motivated by the fact that the ability to control blinking is the last voluntary action of which control is lost \cite{voluntaryblink}. The method relies on facial feature recognition in order to correctly recognise the face and tracking the head position. Having fixed the facial features, eye and nose tip movements are used to steer the cursor, and a blink is used to emulate a click in the user interface.

An important property of the design is the goal of creating a simple, cheap and easily accessible system. In order to achieve simplicity, the system is designed as to not rely on a specifically designed head apparatus as has been suggested in previous systems \cite{goggles1} \cite{goggles2}. To keep the system cheap and accessible, it is designed to rely only on a webcam and the computations necessary to determine the point on the screen at which the user focuses has to take up a minimal amount of processing power of a standard desktop computer. 

This has been achieved by using a Six Segment Rectangular (SSR) filter\cite{ssr}. The idea is to divide the section of the face containing the nose and the eyes into six adjacent rectangular sections; two containing the eyes, one the middle of the eyes, one containing the nose and the remaining two the cheeks. By identifying the nose tip, and the area in the middle of the eyes using face characteristics, the position of the eyes can be reliably found using the fact that the eyes are symmetrically positioned and in a right angle from the line connecting the two points.

For the input method to be usable in as many areas as possible it should not depend on any specific lighting conditions. However the current implementation requires the light to be distributed evenly over the face to avoid false positives when detecting the face. It is not specific for the user, but can be used by anyone after a short face detection process - keeping the system easily accessible.

In order to assess the effectiveness of the system, a set of mathematical formulas for grading were presented. These formulas use the number of false positive, false negative and true positive blinks registered, when comparing registered eye-blinks with actual eye-blinks that can be seen on video-tapings of the sessions. Although the methodology is well-described, there is no section presenting the actual result values of this testing - neither in general nor for specific subjects.

Despite the lack of technical data, it is claimed that the developed system is able to accurately detect facial features (part of the required initialization and re-initialization process) regardless of skin colour and whether the subject is wearing glasses or not. It is also claimed that tracking the face is accurate if the light is evenly distributed on the face and the webcam had an FPS-rate above 20. When the subject was wearing glasses, the system occasionally lost track of the target due to light reflections from the glasses.

\section{Article Quality}
The article is well-written, using a precise and concise language. Abbreviations are avoided unless previously defined and the misspellings in the text are presumably due to the fact that the article has been digitalized from paper. The topic describes the content well and an idea of the content can be formed reading only the title and the abstract. The abstract says little about the drawn results, and the usefulness of the developed system. This reflects the greater flaws in the article as a whole; the result section is not informative enough and no analysis of the usefulness of the system is presented. Although a few alternative systems were mentioned in the article, it would have been preferable if comparison testing had been performed in order to assess the quality of the developed system. From the article it is not indicated if the system is ready to be deployed or if further work is needed.

The presentation of previous related work is relatively rigorous in this paper. Design choices are justified by results of previous research, and in most cases alternative methods are also presented together with a motivation behind the choice made. The same scientific quality is unfortunately not provided in the presentation of results, as it is not backed with measurement data. It is therefore difficult to say whether the conclusions are reasonable or just a matter of opinion.


From this it is clear that the main focus of their project has been the actual implementation of the interface, which is reflected by the detailed technical specifications provided. The text body is well structured with informative section titles, allowing the reader to follow the text in a natural way.


\section{Summary}
The article describes an inexpensive and easy to use system for controlling a computer with eye movements. The system doesn't use any wearable equipment, instead it uses cameras that pinpoint the position of the eyes and nosetip. This information is then used to guide the mouse cursor on the computer, using eye blinks to emulate "clicks". The methodology for deciding if the system is effective is well described and although no data is presented. The input technique described has already been described in earlier work, and with the given information we cannot determine whether the system is better than those earlier described, other than the fact that it is claimed to be effective by the authors themselves.

In general we found this article well-written and helpful as it gave us an good overall understanding of recent research on this subject, and several references to other articles that might be of interest. We believe this article could be of interest to others interested in the topic of facial-feature based interfaces, and we would recommend the interested reader to read at least the introduction section for references and ideas.



\begin{thebibliography}{4}
\bibitem {thepaper} Parmar, K., Mehta, B., Sawant, R. \emph{Facial-Feature Based Human-Computer Interface For Disabled People}. Communication, Information Computing Technology (ICCICT), 2012 International Conference, pp. 1-5.
\bibitem {voluntaryblink} Ruddarraju, R., Haro, A., Nagel, K., Tran, Q. T., Essa, I. A., Abowd, G., Mynatt, E. D. \emph{Perceptual User Interfaces using Vision-based Eye Tracking}. In: the 5th international conference on Multimodal interfaces, Vancouver, British Columbia, Canada, 2003, pp. 227 – 233.
\bibitem {goggles1} J.H. Goldberg and J.C. Schryver. \emph{Eye-gaze determination of user intent at computer interface}. Elsevier Science Publishing, New York, 1995.
\bibitem {goggles2} A. Aaltonen, A. Hyrskykari, and K. Raiha. \emph{101 spots on how do users read menus?}. In: Human Factors in Computing Systems: CHI 98, New York, 1998. ACM Press, pp. 132–139.
\bibitem {ssr} Sawettanusorn, O., Senda, Y., Kawato, S., Tetsutani, N., Yamauchi, H. \emph{Detection of face representative using newly proposed filter}. In: Journal of Signal Processing, 8(2), pp. 137-145.

\end{thebibliography}

\end{document}